{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fabfe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a66c37a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities to compute Conditional Flame Length (CFL) from FLP_English.csv files\n",
    "# and (optionally) rasterize CFL using XPos/YPos as a grid.\n",
    "#\n",
    "# Assumptions:\n",
    "# - Columns: XPos, YPos, PBurn, FIL1..FIL6.  FILk are probabilities for the 6 flame-length bins.\n",
    "# - Midpoints (feet) for the 6 classes: [1, 3, 5, 7, 10, 14]\n",
    "# - Some datasets store FILk as conditional-on-burn probabilities that sum to ~1; others\n",
    "#   store unconditional probabilities that sum to ~PBurn. We'll auto-detect row-by-row.\n",
    "#\n",
    "# Output:\n",
    "# - DataFrame with CFL_ft and a flag for how CFL was computed (conditional vs unconditional),\n",
    "# - Optional GeoTIFF written if CRS is provided or inferred.\n",
    "#\n",
    "# NOTE: This cell defines functions only. Nothing is executed on your files here.\n",
    "\n",
    "import os\n",
    "from typing import Iterable, Tuple, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import rasterio\n",
    "from rasterio.transform import Affine\n",
    "\n",
    "\n",
    "\n",
    "# Fixed midpoints for the 6 flame length classes, in feet\n",
    "FL_MIDPOINTS_FT = np.array([1.0, 3.0, 5.0, 7.0, 10.0, 14.0], dtype=np.float64)\n",
    "\n",
    "\n",
    "def _auto_detect_conditional(row: pd.Series, fil_cols: Iterable[str], pburn_col: str = \"PBurn\", tol: float = 1e-3) -> str:\n",
    "    \"\"\"\n",
    "    Heuristic to determine whether FIL columns are conditional probabilities (sum≈1)\n",
    "    or unconditional probabilities (sum≈PBurn). Returns one of:\n",
    "    - 'conditional'\n",
    "    - 'unconditional'\n",
    "    - 'unknown' (fallback to conditional)\n",
    "    \"\"\"\n",
    "    fil_sum = float(row[fil_cols].sum())\n",
    "    pb = float(row[pburn_col])\n",
    "\n",
    "    # guard against weird/empty rows\n",
    "    if pb < tol and fil_sum < tol:\n",
    "        return \"unknown\"\n",
    "\n",
    "    if abs(fil_sum - 1.0) <= max(tol, 0.05):  # allow some slack for rounding/noise\n",
    "        return \"conditional\"\n",
    "\n",
    "    if abs(fil_sum - pb) <= max(tol, 0.05):\n",
    "        return \"unconditional\"\n",
    "\n",
    "    # If neither is close, choose the closer one\n",
    "    if abs(fil_sum - 1.0) < abs(fil_sum - pb):\n",
    "        return \"conditional\"\n",
    "    else:\n",
    "        return \"unconditional\"\n",
    "\n",
    "\n",
    "def process_csv(csv_path: str,\n",
    "                x_col: str = \"XPos\",\n",
    "                y_col: str = \"YPos\",\n",
    "                pburn_col: str = \"PBurn\",\n",
    "                fil_cols: Iterable[str] = (\"FIL1\", \"FIL2\", \"FIL3\", \"FIL4\", \"FIL5\", \"FIL6\"),\n",
    "                midpoint_ft: np.ndarray = FL_MIDPOINTS_FT,\n",
    "                drop_na_rows: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read one FLP_English.csv and compute Conditional Flame Length (CFL) in feet.\n",
    "    Auto-detects if FIL columns are conditional or unconditional per row.\n",
    "\n",
    "    Returns a DataFrame with columns: [XPos, YPos, PBurn, CFL_ft, mode]\n",
    "      - mode is 'conditional' or 'unconditional' indicating how that row was interpreted.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"\\nColumns:\", df.columns.tolist())\n",
    "    # Basic validation\n",
    "    needed = [x_col, y_col, pburn_col, *fil_cols]\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns in {os.path.basename(csv_path)}: {missing}\")\n",
    "\n",
    "    if drop_na_rows:\n",
    "        df = df.dropna(subset=[*fil_cols, pburn_col])\n",
    "\n",
    "    # Ensure numeric types for calculations\n",
    "    df[fil_cols] = df[list(fil_cols)].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "    df[pburn_col] = pd.to_numeric(df[pburn_col], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Vectorized computation:\n",
    "    fil_vals = df.loc[:, fil_cols].to_numpy(dtype=np.float64)  # (n, 6)\n",
    "    fil_sum = fil_vals.sum(axis=1)                              # (n,)\n",
    "\n",
    "    # Compute two candidate CFLs\n",
    "    cfl_if_conditional = (fil_vals @ midpoint_ft)  # sum(FILk * mid_k)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        cfl_if_unconditional = np.divide((fil_vals @ midpoint_ft),\n",
    "                                         df[pburn_col].to_numpy(dtype=np.float64),\n",
    "                                         out=np.zeros_like(fil_sum, dtype=np.float64),\n",
    "                                         where=df[pburn_col].to_numpy(dtype=np.float64) > 0)\n",
    "\n",
    "    # Auto-pick per row using the heuristic\n",
    "    modes = []\n",
    "    cfl_result = np.empty_like(cfl_if_conditional)\n",
    "    for i in range(len(df)):\n",
    "        mode = _auto_detect_conditional(df.iloc[i], fil_cols=fil_cols, pburn_col=pburn_col)\n",
    "        modes.append(mode)\n",
    "        cfl_result[i] = cfl_if_conditional[i] if mode == \"conditional\" else cfl_if_unconditional[i]\n",
    "\n",
    "    out = df[[x_col, y_col, pburn_col]].copy()\n",
    "    out.rename(columns={x_col: \"XPos\", y_col: \"YPos\", pburn_col: \"PBurn\"}, inplace=True)\n",
    "    out[\"CFL_ft\"] = cfl_result\n",
    "    out[\"mode\"] = modes\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def infer_grid_params(df_xy: pd.DataFrame,\n",
    "                      x_col: str = \"XPos\",\n",
    "                      y_col: str = \"YPos\") -> Tuple[np.ndarray, np.ndarray, float, float]:\n",
    "    \"\"\"\n",
    "    Infer a regular grid from XPos/YPos by finding unique sorted coordinates and\n",
    "    the modal spacing (dx, dy). Returns:\n",
    "      unique_x, unique_y, dx, dy\n",
    "    \"\"\"\n",
    "    xs = np.sort(df_xy[x_col].unique())\n",
    "    ys = np.sort(df_xy[y_col].unique())\n",
    "\n",
    "    if len(xs) < 2 or len(ys) < 2:\n",
    "        raise ValueError(\"Not enough unique X/Y positions to form a grid.\")\n",
    "\n",
    "    dxs = np.diff(xs)\n",
    "    dys = np.diff(ys)\n",
    "\n",
    "    # Use the modal spacing (most common step) to be robust to jitter\n",
    "    def modal_step(arr: np.ndarray) -> float:\n",
    "        vals, counts = np.unique(np.round(arr, 6), return_counts=True)  # round for floating noise\n",
    "        return float(vals[np.argmax(counts)])\n",
    "\n",
    "    dx = modal_step(dxs)\n",
    "    dy = modal_step(dys)\n",
    "\n",
    "    return xs, ys, dx, dy\n",
    "\n",
    "\n",
    "def df_to_geotiff(df_cfl: pd.DataFrame,\n",
    "                  out_path: str,\n",
    "                  crs_epsg: Optional[int] = None,\n",
    "                  x_col: str = \"XPos\",\n",
    "                  y_col: str = \"YPos\",\n",
    "                  value_col: str = \"CFL_ft\",\n",
    "                  nodata: float = np.nan) -> str:\n",
    "    \"\"\"\n",
    "    Rasterize CFL values to a GeoTIFF using the regular grid implied by XPos/YPos.\n",
    "    If crs_epsg is None, a 'local' CRS will be used (GeoTIFF written without CRS).\n",
    "\n",
    "    Returns the output path.\n",
    "    \"\"\"\n",
    "    if rasterio is None:\n",
    "        raise RuntimeError(\"rasterio is not available in this environment. Install rasterio to write GeoTIFFs.\")\n",
    "\n",
    "    xs, ys, dx, dy = infer_grid_params(df_cfl, x_col=x_col, y_col=y_col)\n",
    "\n",
    "    # Build array with origin at top-left (minX, maxY)\n",
    "    x_to_idx = {x: i for i, x in enumerate(xs)}\n",
    "    y_to_idx = {y: i for i, y in enumerate(ys)}\n",
    "\n",
    "    ncols = len(xs)\n",
    "    nrows = len(ys)\n",
    "\n",
    "    # Initialize with nodata\n",
    "    arr = np.full((nrows, ncols), np.nan, dtype=np.float32)\n",
    "\n",
    "    # Fill array: rows from top to bottom should correspond to maxY->minY\n",
    "    # We'll map row index as: row = (nrows - 1 - y_to_idx[y])\n",
    "    for _, row in df_cfl.iterrows():\n",
    "        xi = x_to_idx[row[x_col]]\n",
    "        yi = y_to_idx[row[y_col]]\n",
    "        ri = (nrows - 1 - yi)\n",
    "        ci = xi\n",
    "        arr[ri, ci] = float(row[value_col])\n",
    "\n",
    "    # Define geotransform (Affine): top-left corner of the top-left pixel\n",
    "    # Using half-cell offset so that cell centers align to XPos/YPos\n",
    "    min_x = xs.min()\n",
    "    max_y = ys.max()\n",
    "    transform = Affine.translation(min_x - dx / 2.0, max_y + dy / 2.0) * Affine.scale(dx, -dy)\n",
    "\n",
    "    profile = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": nrows,\n",
    "        \"width\": ncols,\n",
    "        \"count\": 1,\n",
    "        \"dtype\": \"float32\",\n",
    "        \"transform\": transform,\n",
    "        \"compress\": \"lzw\",\n",
    "        \"nodata\": nodata,\n",
    "        \"tiled\": True,\n",
    "        \"interleave\": \"band\"\n",
    "    }\n",
    "    if crs_epsg is not None:\n",
    "        profile[\"crs\"] = f\"EPSG:{crs_epsg}\"\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    with rasterio.open(out_path, \"w\", **profile) as dst:\n",
    "        dst.write(arr, 1)\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def batch_process_folder(root_dir: str,\n",
    "                         pattern_filename: str = \"FLP_English.csv\",\n",
    "                         write_geotiffs: bool = False,\n",
    "                         out_dir: Optional[str] = None,\n",
    "                         crs_epsg: Optional[int] = None) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Walk through `root_dir`, find all `pattern_filename` files, compute CFL, and\n",
    "    (optionally) write a GeoTIFF per CSV.\n",
    "\n",
    "    Returns a dict: {csv_path: df_with_CFL}\n",
    "    \"\"\"\n",
    "    results: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    for current_dir, _, files in os.walk(root_dir):\n",
    "        for f in files:\n",
    "            if f == pattern_filename:\n",
    "                csv_path = os.path.join(current_dir, f)\n",
    "                try:\n",
    "                    df_out = process_csv(csv_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Skipping {csv_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                results[csv_path] = df_out\n",
    "\n",
    "                if write_geotiffs:\n",
    "                    if out_dir is None:\n",
    "                        out_dir = os.path.join(root_dir, \"_CFL_geotiffs\")\n",
    "                    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "                    rel = os.path.relpath(current_dir, root_dir).replace(os.sep, \"_\")\n",
    "                    base = f\"CFL_{rel if rel != '.' else 'root'}.tif\"\n",
    "                    out_path = os.path.join(out_dir, base)\n",
    "                    try:\n",
    "                        df_to_geotiff(df_out, out_path=out_path, crs_epsg=crs_epsg)\n",
    "                        print(f\"[OK] Wrote {out_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"[WARN] Could not write GeoTIFF for {csv_path}: {e}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ad90f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch: make `process_csv` robust to header whitespace and minor naming variations.\n",
    "# It now normalizes column names by stripping spaces and removing internal whitespace,\n",
    "# and it will auto-detect FIL1..FIL6 even if written like \" FIL 1\" etc.\n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def _normalize_columns(cols: Iterable[str]) -> List[str]:\n",
    "    out = []\n",
    "    for c in cols:\n",
    "        # strip outer whitespace and remove inner spaces/tabs\n",
    "        cc = re.sub(r\"\\s+\", \"\", str(c).strip())\n",
    "        out.append(cc)\n",
    "    return out\n",
    "\n",
    "def _resolve_columns(df: pd.DataFrame,\n",
    "                     x_col: str,\n",
    "                     y_col: str,\n",
    "                     pburn_col: str,\n",
    "                     fil_cols: Iterable[str]) -> Tuple[str, str, str, Tuple[str, ...]]:\n",
    "    \"\"\"\n",
    "    Resolve actual column names in df for the requested logical columns.\n",
    "    Works case-insensitively and ignores whitespace differences.\n",
    "    \"\"\"\n",
    "    norm_map = {}  # normalized -> actual\n",
    "    for actual in df.columns:\n",
    "        norm = re.sub(r\"\\s+\", \"\", actual.strip()).lower()\n",
    "        norm_map[norm] = actual\n",
    "\n",
    "    def find_one(target: str, aliases: Iterable[str]) -> str:\n",
    "        # try normalized exact matches against provided aliases (already lowercase/no spaces)\n",
    "        for a in aliases:\n",
    "            if a in norm_map:\n",
    "                return norm_map[a]\n",
    "        # if missing, raise with context\n",
    "        raise KeyError(target)\n",
    "\n",
    "    # candidate aliases (lowercased & whitespace removed)\n",
    "    x_aliases = [x_col, x_col.lower(), \"x\", \"xpos\", \"lon\", \"longitude\"]\n",
    "    y_aliases = [y_col, y_col.lower(), \"y\", \"ypos\", \"lat\", \"latitude\"]\n",
    "    pb_aliases = [pburn_col, pburn_col.lower(), \"pb\", \"pburn\", \"burnprob\", \"burnprobability\"]\n",
    "\n",
    "    # normalize alias tokens\n",
    "    x_aliases = [re.sub(r\"\\s+\", \"\", a).lower() for a in x_aliases]\n",
    "    y_aliases = [re.sub(r\"\\s+\", \"\", a).lower() for a in y_aliases]\n",
    "    pb_aliases = [re.sub(r\"\\s+\", \"\", a).lower() for a in pb_aliases]\n",
    "\n",
    "    # resolve X, Y, PBurn\n",
    "    resolved_x = find_one(\"XPos\", x_aliases)\n",
    "    resolved_y = find_one(\"YPos\", y_aliases)\n",
    "    resolved_pb = find_one(\"PBurn\", pb_aliases)\n",
    "\n",
    "    # resolve FIL columns: allow \"FIL1\", \"FIL 1\", \" FL1 \" etc.\n",
    "    resolved_fils = []\n",
    "    for k in range(1, 7):\n",
    "        candidates = [f\"fil{k}\", f\"fl{k}\", f\"flp{k}\", f\"fil_{k}\", f\"fl_{k}\"]\n",
    "        candidates = [re.sub(r\"\\s+\", \"\", c).lower() for c in candidates]\n",
    "        # direct mapping if exact exists\n",
    "        found = None\n",
    "        for c in candidates:\n",
    "            if c in norm_map:\n",
    "                found = norm_map[c]\n",
    "                break\n",
    "        if found is None:\n",
    "            # brute-force: search any column where normalized matches r\"^fil\\s*0*k$\"\n",
    "            for norm, actual in norm_map.items():\n",
    "                if re.fullmatch(rf\"(fil|fl|flp)_?0*{k}\", norm):\n",
    "                    found = actual\n",
    "                    break\n",
    "        if found is None:\n",
    "            raise KeyError(f\"FIL{k}\")\n",
    "        resolved_fils.append(found)\n",
    "\n",
    "    return resolved_x, resolved_y, resolved_pb, tuple(resolved_fils)\n",
    "\n",
    "\n",
    "def process_csv(csv_path: str,\n",
    "                x_col: str = \"XPos\",\n",
    "                y_col: str = \"YPos\",\n",
    "                pburn_col: str = \"PBurn\",\n",
    "                fil_cols: Iterable[str] = (\"FIL1\", \"FIL2\", \"FIL3\", \"FIL4\", \"FIL5\", \"FIL6\"),\n",
    "                midpoint_ft: np.ndarray = FL_MIDPOINTS_FT,\n",
    "                drop_na_rows: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read one FLP_English.csv and compute Conditional Flame Length (CFL) in feet.\n",
    "    Robust to header whitespace and minor naming variants (e.g., ' FIL1', 'FIL 1', 'fl1').\n",
    "\n",
    "    Returns a DataFrame with columns: [XPos, YPos, PBurn, CFL_ft, mode]\n",
    "    \"\"\"\n",
    "    # Use engine='python' to be tolerant; strip leading spaces after delimiter.\n",
    "    df = pd.read_csv(csv_path, engine=\"python\", skipinitialspace=True)\n",
    "\n",
    "    # Show original columns for debugging\n",
    "    print(\"\\n[DEBUG] Raw columns:\", list(df.columns))\n",
    "\n",
    "    # Normalize names for matching, but keep original df with original names.\n",
    "    # We'll resolve actual names and then use those.\n",
    "    try:\n",
    "        resolved_x, resolved_y, resolved_pb, resolved_fils = _resolve_columns(\n",
    "            df, x_col=x_col, y_col=y_col, pburn_col=pburn_col, fil_cols=fil_cols\n",
    "        )\n",
    "    except KeyError as ke:\n",
    "        # Provide a friendlier error that shows what's present and what's missing\n",
    "        missing = str(ke).strip(\"'\")\n",
    "        raise ValueError(\n",
    "            f\"Missing required column '{missing}' in {os.path.basename(csv_path)}.\\n\"\n",
    "            f\"Seen columns: {list(df.columns)}\\n\"\n",
    "            f\"Tip: headers sometimes contain leading spaces — this function now tries to fix that. \"\n",
    "            f\"If it still fails, please share a sample header line.\"\n",
    "        )\n",
    "\n",
    "    if drop_na_rows:\n",
    "        df = df.dropna(subset=[*resolved_fils, resolved_pb])\n",
    "\n",
    "    # Ensure numeric\n",
    "    df[list(resolved_fils)] = df[list(resolved_fils)].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "    df[resolved_pb] = pd.to_numeric(df[resolved_pb], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Vectorized computation\n",
    "    fil_vals = df.loc[:, resolved_fils].to_numpy(dtype=np.float64)\n",
    "    fil_sum = fil_vals.sum(axis=1)\n",
    "    cfl_if_conditional = (fil_vals @ midpoint_ft)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        cfl_if_unconditional = np.divide((fil_vals @ midpoint_ft),\n",
    "                                         df[resolved_pb].to_numpy(dtype=np.float64),\n",
    "                                         out=np.zeros_like(fil_sum, dtype=np.float64),\n",
    "                                         where=df[resolved_pb].to_numpy(dtype=np.float64) > 0)\n",
    "\n",
    "    # Decide mode per row\n",
    "    modes = []\n",
    "    cfl_result = np.empty_like(cfl_if_conditional)\n",
    "    # Build a temp df view with resolved names to reuse the same heuristic\n",
    "    tmp = df[[resolved_pb, *resolved_fils]].copy()\n",
    "    tmp.columns = [\"PBurn\"] + [f\"FIL{k}\" for k in range(1, 7)]\n",
    "    for i in range(len(tmp)):\n",
    "        mode = _auto_detect_conditional(tmp.iloc[i], fil_cols=[f\"FIL{k}\" for k in range(1, 7)], pburn_col=\"PBurn\")\n",
    "        modes.append(mode)\n",
    "        cfl_result[i] = cfl_if_conditional[i] if mode == \"conditional\" else cfl_if_unconditional[i]\n",
    "\n",
    "    # Build output with canonical names\n",
    "    out = pd.DataFrame({\n",
    "        \"XPos\": df[resolved_x].values,\n",
    "        \"YPos\": df[resolved_y].values,\n",
    "        \"PBurn\": df[resolved_pb].values,\n",
    "        \"CFL_ft\": cfl_result,\n",
    "        \"mode\": modes\n",
    "    })\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "877a0afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] Raw columns: ['XPos', 'YPos', 'PBurn', 'FIL1', 'FIL2', 'FIL3', 'FIL4', 'FIL5', 'FIL6']\n",
      "\n",
      "[DEBUG] Raw columns: ['XPos', 'YPos', 'PBurn', 'FIL1', 'FIL2', 'FIL3', 'FIL4', 'FIL5', 'FIL6']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m root_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbsf31\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mNL060\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWFM Outputs\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrun_97th_percentiles\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 1) Process all subfolders, just get DataFrames back\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m batch_process_folder(root_dir)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Each key is a CSV path; each value is a DataFrame with columns: XPos, YPos, PBurn, CFL_ft, mode\u001b[39;00m\n\u001b[0;32m      7\u001b[0m df_one \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(results\u001b[38;5;241m.\u001b[39mvalues()))\n",
      "Cell \u001b[1;32mIn[4], line 227\u001b[0m, in \u001b[0;36mbatch_process_folder\u001b[1;34m(root_dir, pattern_filename, write_geotiffs, out_dir, crs_epsg)\u001b[0m\n\u001b[0;32m    225\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_dir, f)\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     df_out \u001b[38;5;241m=\u001b[39m process_csv(csv_path)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[WARN] Skipping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 136\u001b[0m, in \u001b[0;36mprocess_csv\u001b[1;34m(csv_path, x_col, y_col, pburn_col, fil_cols, midpoint_ft, drop_na_rows)\u001b[0m\n\u001b[0;32m    134\u001b[0m tmp\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPBurn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFIL\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m)]\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tmp)):\n\u001b[1;32m--> 136\u001b[0m     mode \u001b[38;5;241m=\u001b[39m _auto_detect_conditional(tmp\u001b[38;5;241m.\u001b[39miloc[i], fil_cols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFIL\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m)], pburn_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPBurn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m     modes\u001b[38;5;241m.\u001b[39mappend(mode)\n\u001b[0;32m    138\u001b[0m     cfl_result[i] \u001b[38;5;241m=\u001b[39m cfl_if_conditional[i] \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconditional\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m cfl_if_unconditional[i]\n",
      "Cell \u001b[1;32mIn[4], line 39\u001b[0m, in \u001b[0;36m_auto_detect_conditional\u001b[1;34m(row, fil_cols, pburn_col, tol)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_auto_detect_conditional\u001b[39m(row: pd\u001b[38;5;241m.\u001b[39mSeries, fil_cols: Iterable[\u001b[38;5;28mstr\u001b[39m], pburn_col: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPBurn\u001b[39m\u001b[38;5;124m\"\u001b[39m, tol: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     32\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m    Heuristic to determine whether FIL columns are conditional probabilities (sum≈1)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m    or unconditional probabilities (sum≈PBurn). Returns one of:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    - 'unknown' (fallback to conditional)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     fil_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(row[fil_cols]\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m     40\u001b[0m     pb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(row[pburn_col])\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# guard against weird/empty rows\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bsf31\\miniconda3\\envs\\dev_env\\Lib\\site-packages\\pandas\\core\\series.py:1153\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rows_with_mask(key)\n\u001b[1;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_with(key)\n",
      "File \u001b[1;32mc:\\Users\\bsf31\\miniconda3\\envs\\dev_env\\Lib\\site-packages\\pandas\\core\\series.py:1194\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[key]\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;66;03m# handle the dup indexing case GH#4246\u001b[39;00m\n\u001b[1;32m-> 1194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc[key]\n",
      "File \u001b[1;32mc:\\Users\\bsf31\\miniconda3\\envs\\dev_env\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\bsf31\\miniconda3\\envs\\dev_env\\Lib\\site-packages\\pandas\\core\\indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_iterable(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32mc:\\Users\\bsf31\\miniconda3\\envs\\dev_env\\Lib\\site-packages\\pandas\\core\\indexing.py:1361\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_listlike_indexer(key, axis)\n\u001b[1;32m-> 1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1363\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bsf31\\miniconda3\\envs\\dev_env\\Lib\\site-packages\\pandas\\core\\generic.py:5686\u001b[0m, in \u001b[0;36mNDFrame._reindex_with_indexers\u001b[1;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[0;32m   5683\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m ensure_platform_int(indexer)\n\u001b[0;32m   5685\u001b[0m \u001b[38;5;66;03m# TODO: speed up on homogeneous DataFrame objects (see _reindex_multi)\u001b[39;00m\n\u001b[1;32m-> 5686\u001b[0m new_data \u001b[38;5;241m=\u001b[39m new_data\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m   5687\u001b[0m     index,\n\u001b[0;32m   5688\u001b[0m     indexer,\n\u001b[0;32m   5689\u001b[0m     axis\u001b[38;5;241m=\u001b[39mbaxis,\n\u001b[0;32m   5690\u001b[0m     fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m   5691\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39mallow_dups,\n\u001b[0;32m   5692\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   5693\u001b[0m )\n\u001b[0;32m   5694\u001b[0m \u001b[38;5;66;03m# If we've made a copy once, no need to make another one\u001b[39;00m\n\u001b[0;32m   5695\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bsf31\\miniconda3\\envs\\dev_env\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\bsf31\\miniconda3\\envs\\dev_env\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:773\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    771\u001b[0m             bp \u001b[38;5;241m=\u001b[39m BlockPlacement(\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, sllen))\n\u001b[0;32m    772\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 773\u001b[0m                 blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    774\u001b[0m                     slobj,\n\u001b[0;32m    775\u001b[0m                     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    776\u001b[0m                     new_mgr_locs\u001b[38;5;241m=\u001b[39mbp,\n\u001b[0;32m    777\u001b[0m                     fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    778\u001b[0m                 )\n\u001b[0;32m    779\u001b[0m             ]\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sl_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    782\u001b[0m     blknos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblknos[slobj]\n",
      "File \u001b[1;32mc:\\Users\\bsf31\\miniconda3\\envs\\dev_env\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m algos\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m   1308\u001b[0m     values, indexer, axis\u001b[38;5;241m=\u001b[39maxis, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[0;32m   1309\u001b[0m )\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bsf31\\miniconda3\\envs\\dev_env\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[1;32mc:\\Users\\bsf31\\miniconda3\\envs\\dev_env\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[1;32m--> 162\u001b[0m func(arr, indexer, out, fill_value)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[0;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "root_dir = r\"C:\\Users\\bsf31\\Documents\\data\\NL060\\WFM Outputs\\run_97th_percentiles\"\n",
    "\n",
    "# 1) Process all subfolders, just get DataFrames back\n",
    "results = batch_process_folder(root_dir)\n",
    "\n",
    "# Each key is a CSV path; each value is a DataFrame with columns: XPos, YPos, PBurn, CFL_ft, mode\n",
    "df_one = next(iter(results.values()))\n",
    "df_one.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199da566",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc7d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) (Optional) Also write CFL rasters as GeoTIFFs\n",
    "# Provide your EPSG if you know the coordinates of XPos/YPos (e.g., 26911 for NAD83 / UTM zone 11N)\n",
    "results = batch_process_folder(\n",
    "    root_dir,\n",
    "    write_geotiffs=True,\n",
    "    crs_epsg=26911  # <- change if needed, or None to write without CRS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5829372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28342dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    XPos   YPos  PBurn  FIL1  FIL2  FIL3  FIL4  FIL5  FIL6\n",
      "0  100.0  200.0   0.75  0.10  0.20  0.30  0.20  0.15  0.05\n",
      "1  110.0  200.0   0.60  0.06  0.12  0.18  0.12  0.09  0.03\n",
      "2  100.0  210.0   0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
      "3  110.0  210.0   0.90  0.05  0.15  0.25  0.25  0.15  0.15\n",
      "\n",
      "Columns: ['XPos', 'YPos', 'PBurn', 'FIL1', 'FIL2', 'FIL3', 'FIL4', 'FIL5', 'FIL6']\n"
     ]
    }
   ],
   "source": [
    "# create an example FLP-style dataframe and show its columns\n",
    "df_example = pd.DataFrame([\n",
    "    # conditional-style row (FIL sum ~1.0, PBurn is prob of burn)\n",
    "    {\"XPos\": 100.0, \"YPos\": 200.0, \"PBurn\": 0.75, \"FIL1\": 0.10, \"FIL2\": 0.20, \"FIL3\": 0.30, \"FIL4\": 0.20, \"FIL5\": 0.15, \"FIL6\": 0.05},\n",
    "    # unconditional-style row (FIL sum ~PBurn)\n",
    "    {\"XPos\": 110.0, \"YPos\": 200.0, \"PBurn\": 0.60, \"FIL1\": 0.06, \"FIL2\": 0.12, \"FIL3\": 0.18, \"FIL4\": 0.12, \"FIL5\": 0.09, \"FIL6\": 0.03},\n",
    "    # zero-burn row\n",
    "    {\"XPos\": 100.0, \"YPos\": 210.0, \"PBurn\": 0.0,  \"FIL1\": 0.0,  \"FIL2\": 0.0,  \"FIL3\": 0.0,  \"FIL4\": 0.0,  \"FIL5\": 0.0,  \"FIL6\": 0.0},\n",
    "    # another conditional-style row\n",
    "    {\"XPos\": 110.0, \"YPos\": 210.0, \"PBurn\": 0.90, \"FIL1\": 0.05, \"FIL2\": 0.15, \"FIL3\": 0.25, \"FIL4\": 0.25, \"FIL5\": 0.15, \"FIL6\": 0.15},\n",
    "])\n",
    "\n",
    "# show the dataframe and its columns\n",
    "print(df_example)\n",
    "print(\"\\nColumns:\", df_example.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
